# vector_manager.py

import os
import json
from langchain_community.embeddings import OpenAIEmbeddings  # Updated import
from langchain_community.vectorstores import Chroma  # Updated import
from langchain.docstore.document import Document
from google_search import google_partselect_search
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - VectorManager - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("vector_manager.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("VectorManager")


class LivePartSelectMemory:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LivePartSelectMemory, cls).__new__(cls)
            cls._instance.initialize()
        return cls._instance

    def initialize(self):
        """
        Initializes the embedding model and vector store. This method is called only once.
        """
        self.embedding_model = OpenAIEmbeddings(
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            model="text-embedding-ada-002"  # Update if a more recent model is available
        )

        # Initialize Chroma without persistence
        self.vector_store = Chroma(
            embedding_function=self.embedding_model,
            collection_name="partselect_data"
        )

        logger.info("‚úÖ LivePartSelectMemory initialized with non-persistent ChromaDB.")

    def live_search_and_index(self, query: str, k=3):
        """
        1) Perform a live Google search for the query (restricted to PartSelect).
        2) Extract snippets & links.
        3) Convert them into Document objects and store them in the vector store.
        4) Returns a confirmation message.
        """
        results = google_partselect_search(query, num_results=k)
        if not results:
            logger.warning(f"No search results found for: {query}")
            return "No results found on PartSelect for that query."

        documents = []
        for snippet, link in results:
            combined_text = f"Snippet: {snippet}\nURL: {link}"

            if combined_text.strip():  # Ensure valid text before embedding
                doc = Document(page_content=combined_text, metadata={"source": link})
                documents.append(doc)

        if not documents:
            logger.warning("No valid documents found to index.")
            return "No valid data found to index."

        try:
            # Add documents to the vector store (Chroma handles embeddings internally)
            self.vector_store.add_documents(documents)
            logger.info(f"‚úÖ Live indexed {len(documents)} new items from PartSelect.")
            return f"Live indexed {len(documents)} new items from PartSelect."
        except Exception as e:
            logger.exception(f"Error during indexing: {e}")
            return "Error: Unable to index data."

    def semantic_search_with_intent(self, query: str, intent: str, model_number: str = None, top_k: int = 3):
        """
        Performs semantic search based on the query and intent.
        intent can be one of: ['troubleshoot', 'installation', 'compatibility', 'qna', ...].
        Optionally filters by model_number if provided.
        """
        filter_metadata = {}
        if intent == 'troubleshoot':
            filter_metadata = {"type": {"$eq": "user_story"}}
            if model_number:
                filter_metadata["model"] = {"$eq": model_number}  # Filter by model if provided
        elif intent == 'installation':
            filter_metadata = {"type": {"$eq": "installation_guides"}}
            if model_number:
                filter_metadata["model"] = {"$eq": model_number}
        elif intent == 'compatibility':
            filter_metadata = {"type": {"$eq": "model_compatibility"}}
        elif intent == 'qna':
            filter_metadata = {"type": {"$eq": "qna"}}
        else:
            filter_metadata = {}  # No filter for general intent

        logger.debug(f"Performing semantic search with query: '{query}', intent: '{intent}', model: '{model_number}', filter: {filter_metadata}")

        try:
            # Correctly pass the filter to the similarity_search function
            results = self.vector_store.similarity_search(query, k=top_k, filter=filter_metadata)
            logger.debug(f"Semantic search results: {results}")

            if not results:
                logger.warning(f"No relevant results found for query: '{query}' with intent: '{intent}' and model: '{model_number}'")
                return ["No relevant results found for this intent."]

            formatted_results = []
            for doc in results:
                metadata = doc.metadata
                if "title" in metadata and "instruction" in metadata:
                    formatted_results.append({
                        "title": metadata['title'],
                        "instruction": metadata['instruction'],
                        "source": metadata.get('source')
                    })
                elif "part_name" in metadata:
                    formatted_results.append({
                        "part_name": metadata['part_name'],
                        "source": metadata.get('source')
                    })
                else:
                    # Fallback to title if available
                    formatted_results.append({
                        "title": metadata.get('title', 'No Title'),
                        "instruction": metadata.get('instruction', ''),
                        "source": metadata.get('source')
                    })

            logger.info(f"‚úÖ Semantic search returned {len(formatted_results)} results for intent '{intent}'.")
            return formatted_results

        except Exception as e:
            logger.exception(f"‚ùå Semantic search failed: {e}")
            return ["Error occurred during semantic search."]


# Instantiate live_store with Singleton pattern
live_store = LivePartSelectMemory()


def index_scraped_data(json_str: str) -> str:
    """
    Parses JSON data from the scraper and indexes relevant sections into the vector store.
    """
    logger.info(f"Indexing data: {json_str}")
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError:
        logger.error("‚ùå Invalid JSON input.")
        return "‚ùå Invalid JSON input."

    documents = []

    # ============================
    # 1. Q&A Document Tagging
    # ============================
    qna_list = data.get("qna", [])
    if isinstance(qna_list, list):
        for pair in qna_list:
            if isinstance(pair, dict):
                question = pair.get("question", "").strip()
                answer = pair.get("answer", "").strip()
                if question and answer:
                    # üè∑Ô∏è Add metadata={"type": "qna"} for Q&A
                    content = f"Q: {question}\nA: {answer}"
                    documents.append(
                        Document(
                            page_content=content,
                            metadata={
                                "type": "qna",
                                "source": "scraped_json"
                            }
                        )
                    )

    # ============================
    # 2. Troubleshooting Document Tagging
    # ============================
    troubleshooting_info = data.get("troubleshooting_info", {})
    if isinstance(troubleshooting_info, dict):
        # "symptoms" -> "troubleshooting_symptoms"
        symptoms = troubleshooting_info.get("symptoms", [])
        if symptoms:
            content = f"Symptoms: {', '.join(symptoms)}"
            documents.append(
                Document(
                    page_content=content,
                    metadata={
                        "type": "troubleshooting_symptoms",
                        "source": "scraped_json"
                    }
                )
            )

        # "products" -> "troubleshooting_products"
        products = troubleshooting_info.get("products", [])
        if products:
            content = f"Products: {', '.join(products)}"
            documents.append(
                Document(
                    page_content=content,
                    metadata={
                        "type": "troubleshooting_products",
                        "source": "scraped_json"
                    }
                )
            )

        # "replacements" -> "troubleshooting_replacements"
        replacements = troubleshooting_info.get("replacements", [])
        if replacements:
            content = f"Replacements: {', '.join(replacements)}"
            documents.append(
                Document(
                    page_content=content,
                    metadata={
                        "type": "troubleshooting_replacements",
                        "source": "scraped_json"
                    }
                )
            )

    # ============================
    # 3. Model Compatibility Document Tagging
    # ============================
    model_compatibility = data.get("model_compatibility", [])
    if isinstance(model_compatibility, list):
        for model in model_compatibility:
            if isinstance(model, dict):
                brand = model.get("brand", "").strip()
                model_number = model.get("model_number", "").strip()
                description = model.get("description", "").strip()
                if brand and model_number and description:
                    # üè∑Ô∏è Mark as "model_compatibility"
                    content = f"Brand: {brand}\nModel Number: {model_number}\nDescription: {description}"
                    documents.append(
                        Document(
                            page_content=content,
                            metadata={
                                "type": "model_compatibility",
                                "source": "scraped_json"
                            }
                        )
                    )

    # ============================
    # 4. Installation Document Tagging
    # ============================
    installation_info = data.get("installation_info", "").strip()
    if installation_info and installation_info != "No installation information available.":
        documents.append(
            Document(
                page_content=installation_info,
                metadata={
                    "type": "installation_guides",
                    "source": "scraped_json"
                }
            )
        )

    # ============================
    # 5. Data Segmentation (Chunking)
    # ============================
    def split_into_chunks(text, chunk_size=500):
        """
        Splits text into chunks of approximately `chunk_size` characters.
        """
        return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

    # Handle full_description by chunking
    if 'full_description' in data:
        desc_chunks = split_into_chunks(data['full_description'], chunk_size=500)
        for chunk in desc_chunks:
            documents.append(
                Document(
                    page_content=chunk,
                    metadata={
                        "type": "full_description",
                        "source": "scraped_json"
                    }
                )
            )

    # ============================
    # 6. Symptom Information Tagging
    # ============================
    # Index common parts
    for part in data.get("common_parts", []):
        content = f"Part: {part['part_name']}\nFix Percentage: {part['fix_percentage']}%\nPrice: ${part['price']}\nDescription: {part['description']}"
        documents.append(
            Document(
                page_content=content,
                metadata={
                    "type": "part_info",
                    "model": data.get("model_number", ""),
                    "source": "scraped_json"
                }
            )
        )

        # Index user stories: combine title and instruction
        for story in part.get("user_stories", []):
            content = f"Title: {story['title']}\nInstruction: {story['instruction']}"
            documents.append(
                Document(
                    page_content=content,
                    metadata={
                        "type": "user_story",
                        "model": data.get("model_number", ""),
                        "source": "scraped_json"
                    }
                )
            )

    if not documents:
        logger.error("‚ùå No valid documents found to index.")
        return "‚ùå No valid documents found to index."

    try:
        # Add documents to the vector store
        live_store.vector_store.add_documents(documents)
        logger.info(f"‚úÖ Indexed {len(documents)} documents from scraped data.")
        return f"‚úÖ Indexed {len(documents)} documents from scraped data."
    except Exception as e:
        logger.exception(f"‚ùå Error during indexing: {e}")
        return "‚ùå Failed to index scraped data."


def semantic_search_with_intent(query: str, intent: str, model_number: str = None, top_k: int = 3):
    """
    Performs semantic search based on the query and intent.
    """
    return live_store.semantic_search_with_intent(query, intent, model_number, top_k)


# agents.py

import json
import logging
from langchain.schema import HumanMessage, SystemMessage

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - Agent - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("agents.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("Agent")

# ============================
# 1. Define Tools
# ============================

def gpt_tool(query: str) -> str:
    """
    Utilizes GPT to generate responses when necessary.
    """
    messages = [
        SystemMessage(content=(
            "You are a helpful chatbot specialized in PartSelect queries for "
            "refrigerator and dishwasher parts. Use the provided data or your training knowledge, "
            "but remain within that domain. If not relevant to parts, politely decline."
        )),
        HumanMessage(content=query)
    ]

    try:
        # Use local import within functions if needed
        from agent_manager import AgentManager
        response = AgentManager().llm.invoke(messages)  # Use invoke instead of __call__
        return response.content.strip()  # Access content directly
    except Exception as e:
        logger.exception(f"‚ùå GPT tool encountered an error: {e}")
        return "‚ùå Failed to generate response using GPT."

# ============================
# 2. Formatting Functions
# ============================

def format_response(data: dict, intent: str, original_query: str, session: dict) -> str:
    """
    Summarizes `scraped_data` for GPT: includes instructions, user stories, and overall context.
    """
    if "scraped_data" in data:
        scraped_data = data["scraped_data"]

        # We'll build a single dict for GPT containing:
        # - The original user query
        # - The relevant instructions from user stories
        # - Possibly the part's official description
        # - The highest fix_percentage part name
        # - The product link if available
        final_context = {
            "user_query": original_query,
            "intent": intent,
            "model_number": scraped_data.get("model_number", ""),
            "symptom_title": scraped_data.get("symptom_title", ""),
            "troubleshoot_instructions": scraped_data.get("troubleshoot_instructions", []),
            "highest_fix_part_description": "",
            "product_link": scraped_data.get("product_url", ""),
            "highest_fix_part_name": "",
            "fix_percentage": ""
        }

        # If common_parts exist, show the top part's description:
        parts = scraped_data.get("common_parts", [])
        if parts:
            top_part = parts[0]
            final_context["highest_fix_part_description"] = top_part.get("description", "")
            final_context["highest_fix_part_name"] = top_part.get("part_name", "")
            final_context["fix_percentage"] = top_part.get("fix_percentage", "")

        # Prepare the system prompt with conversation history for multi-turn
        history = "\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in session.get('history', [])])

        gpt_prompt = (
            "You are a specialized assistant for refrigerator and dishwasher part issues. "
            "Use the data below to answer the user's question accurately.\n\n"
            f"Conversation history:\n{history}\n\n"
            "Structured Data:\n"
            + json.dumps(final_context, indent=2)
            + "\n\nPlease compose a helpful, concise, markdown-formatted answer focusing on the user's question and the relevant instructions."
        )

        formatted_response = gpt_tool(gpt_prompt)

        # Append product URL at the end of the response
        if final_context["product_link"]:
            formatted_response += f"\n\nüîó **[View Product on PartSelect]({final_context['product_link']})**"

        return formatted_response

    elif "search_results" in data:
        # Summarize search results if needed
        return gpt_tool("Summarize these search results:\n" + json.dumps(data["search_results"]))
    elif "clarifying_question" in data:
        # Return the clarifying question directly
        return data["clarifying_question"]
    else:
        return "‚ùå No relevant information found to generate a response."


